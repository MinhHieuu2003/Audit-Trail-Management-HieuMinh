[
{
	"uri": "//localhost:1313/",
	"title": "Audit Trail Management",
	"tags": [],
	"description": "",
	"content": "Audit Trail Management Overview In this demo, you will learn the basic concepts and hands-on practice with Amazon System Manager - Session Management. You will practice connecting to both public and private servers in a VPC, manage session logs, and understand how to ensure auditability and compliance for your infrastructure.\nContents Introduction Preparation Steps Set Up Audit Automation Deploy Immutable Logging with AWS S3 Analyze Trail with Amazon OpenSearch Service Compliance Reporting with Athena Investigation Tools with Amazon QuickSight Monitoring \u0026amp; Alerts with Amazon CloudWatch Operational Procedures [Legal Coordination with AWS Artifact](10-AWS Artifact/) [Clean Resources](11-Clean resource/) "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/3.1-public-instance/",
	"title": "IAM Role for Lambda",
	"tags": [],
	"description": "",
	"content": "Create IAM Role Go to the IAM Role management interface. Step 1: Select AWS service.\nSelect Lambda. Click Next. Step 2: Enter AWSLambdaBasicExecutionRole and check it.\nAdd a custom policy to allow qldb:SendCommand and qldb:InsertDocument (if writing directly to QLDB). Step 3: Name the role: AuditLoggerLambdaRole\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-createiamrole/2.2.1-keypair/",
	"title": "Key Pair",
	"tags": [],
	"description": "",
	"content": "Create Key Pair Click Key Pairs. Click Create Key pair. On the Create Key pair page: For Name, enter audit-demo-key. For Format, select .pem. Scroll to the bottom of the page and click Create key pair.\nDownload the file audit-demo-key.pem to your computer and set permissions with chmod 400 audit-demo-key.pem\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/2.1.1-createvpc/",
	"title": "Create VPC",
	"tags": [],
	"description": "",
	"content": "Create VPC Lab VPC Go to the [VPC service management interface] Click Your VPCs. Click Create VPC. On the Create VPC page: In the Name tag field, enter AuditDemo-VPC. In the IPv4 CIDR field, enter: 10.10.0.0/16. Click Create VPC. "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/",
	"title": "Create VPC and Network Components",
	"tags": [],
	"description": "",
	"content": "Prepare AWS Free Tier Account Register at https://aws.amazon.com/free and verify your credit card information.\nLog in to the Console, select a Region (for example: Asia Pacific (Singapore) ap-southeast-1) to reduce latency.\nThe overall architecture after you complete this step will look like:\nTo learn how to create VPCs with public/private subnets, you can refer to the lab:\nWorking with Amazon VPC Contents Create VPC Create Public subnet Create Private subnet Create security group Create public Linux server Create private Windows server "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Audit Trail Management with Immutable Logging is a solution designed to record all user and system activities in a continuous, accurate, and tamper-proof manner. In today\u0026rsquo;s IT environment, where security, compliance, and traceability are critical requirements, implementing audit trails is indispensable—especially in systems with high demands for monitoring and auditing.\nHowever, traditional logging systems can be vulnerable to data manipulation or deletion if not properly protected. Therefore, Immutable Logging emerges as a solution to ensure that every recorded action cannot be altered or forged—serving as a reliable foundation for inspection, monitoring, and post-incident investigation.\nBy adopting Audit Trail Management combined with Immutable Logging, you gain the following advantages: Logs cannot be modified or deleted after being recorded—ensuring data integrity.\nComplete capture of user and system activities (who did what, when, and where). Easy integration with SIEM systems, security analysis tools, or audit platforms. Long-term log retention that supports compliance with standards such as ISO 27001, GDPR, and HIPAA. Enhanced incident investigation capabilities, enabling tracing of internal violations or external attacks. Cost-optimized compared to traditional methods like manual log backups. Can be implemented using technologies such as Blockchain, WORM storage, or append-only logging systems. Builds transparency and trust with clients and partners. With these advantages, Audit Trail with Immutable Logging is an ideal choice for organizations seeking to strengthen system monitoring, ensure regulatory compliance, and proactively mitigate security risks in a reliable and effective way.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-createiamrole/",
	"title": "Create Key Pair and Launch EC2 Instance",
	"tags": [],
	"description": "",
	"content": "Create Public Subnet Click Subnets. Click Create subnet. On the Create subnet page: For VPC ID, select AuditDemo-PublicSubnet. For Subnet name, enter AuditDemo-PublicSubnet. For Availability Zone, select the first Availability Zone. For IPv4 CIDR block, enter 10.10.1.0/24. Scroll to the bottom of the page and click Create subnet. "
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "\rYou need to create one Linux instance in the public subnet and one Windows instance in the private subnet in advance to perform this lab.\nTo learn how to create EC2 instances and VPCs with public/private subnets, you can refer to the following labs:\nIntroduction to Amazon EC2 Working with Amazon VPC To use System Manager to manage Windows instances in particular and all your instances on AWS in general, you need to grant permissions for your instances to work with System Manager. In this preparation section, we will also create an IAM Role to allow the instances to work with System Manager.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/2.1.2-createpublicsubnet/",
	"title": "Create Public Subnet",
	"tags": [],
	"description": "",
	"content": "Create Public Subnet Click Subnets. Click Create subnet. On the Create subnet page: For VPC ID, select AuditDemo-PublicSubnet. For Subnet name, enter AuditDemo-PublicSubnet. For Availability Zone, select the first Availability Zone. For IPv4 CIDR block, enter 10.10.1.0/24. Scroll to the bottom of the page and click Create subnet. "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/3.3-console/",
	"title": "Deploy Lambda via AWS CLI or Console",
	"tags": [],
	"description": "",
	"content": "Create an AWS Lambda Function Via Console: AWS Console: navigate to Lambda → Functions → Create function. Function name: AuditLoggerDemo Runtime: Python 3.9 Architecture: x86_64 Permissions: Open Change default execution role → Select Use an existing role -\u0026gt; AuditLoggerLambdaRole. Click Create function. On the new function page: Code Source: Select Upload from → .zip file. Upload the function.zip file (containing audit_logger.py). Deploy the code. Test event In the Test tab Set Event name: TestLogin Paste the JSON: {\u0026ldquo;user\u0026rdquo;:\u0026ldquo;hocsinh1\u0026rdquo;,\u0026ldquo;action\u0026rdquo;:\u0026ldquo;login\u0026rdquo;,\u0026ldquo;timestamp\u0026rdquo;:\u0026ldquo;2025-07-27T12:00:00Z\u0026rdquo;} Run the Function With the TestLogin event selected, click Test. The execution result will be displayed below: Configure CloudWatch Logs Check the Log Group. Console \u0026gt; CloudWatch \u0026gt; Logs \u0026gt; Log groups \u0026gt; /aws/lambda/AuditLoggerDemo. Each invocation will create a new log stream. Create Metric Filter (\u0026hellip;) "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/",
	"title": "Set Up Audit Automation",
	"tags": [],
	"description": "",
	"content": "In this step, we will deploy a Lambda function to automatically record audit logs, configure log collection, and display them in CloudWatch.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/2.1.3-createprivatesubnet/",
	"title": "Create Internet Gateway &amp; Route Table",
	"tags": [],
	"description": "",
	"content": "Create Private Subnet Click Internet Gateways. Configure Internet Gateways. In the Name tag, enter AuditDemo-IGW. Click Create internet gateway. Confirm successful creation of Internet Gateway Confirm successful creation of Internet Gateway Connect to VPC\n4. Attach Internet Gateway to VPC\nClick Actions Select Attach to AuditDemo-VPC Select VPC ASG from the list (VPC ID will be filled automatically) Click Attach internet gateway Create Route Tables Create Route Table in Amazon VPC\nGo to the VPC interface Select Route Tables from the left menu Click Create route table Configure Route Table Name: AuditDemo-RT VPC: AuditDemo-VPC Click Create route table In the Route Table Click Edit Routes → Add route Destination: 0.0.0.0/0 Target: Internet Gateway -\u0026gt; igw-(AuditDemo-IGW) Click Save routes "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-createec2/2.1.4-createsecgroup/",
	"title": "Create Security Groups",
	"tags": [],
	"description": "",
	"content": "Create Security Groups In this step, we will create the security groups used for our instances. As you can see, these security groups do not need to open traditional ports for SSH (port 22) or Remote Desktop (port 3389).\nCreate a security group for the Linux instance in the public subnet Go to the VPC service management interface Click Security Group. Click Create security group. 2. Click Create security group\nIn the Security group name field, enter AuditDemo-SG. In the Description field, enter AuditDemo. In the VPC field, select AuditDemo-VPC. Set Inbound Rules: SSH (TCP 22) source My IP. HTTPS (TCP 443) source 0.0.0.0/0 Outbound: Keep the default All traffic Click Create security group. This completes the creation of the necessary security groups for EC2 instances and the VPC Endpoint. "
},
{
	"uri": "//localhost:1313/4-s3log/",
	"title": "Manage session logs",
	"tags": [],
	"description": "",
	"content": "With Session Manager, we can view the history of connections to instances through Session history. However, we have not seen the details of the commands used in a session.\nIn this section, we will proceed to create an S3 bucket and configure the session logs feature to see the details of the commands used in the session.\nContent: Update IAM Role Create S3 Bucket Create S3 Gateway endpoint Configure Session logs "
},
{
	"uri": "//localhost:1313/5-portfwd/",
	"title": "Analyze Trail with Amazon OpenSearch Service",
	"tags": [],
	"description": "",
	"content": "Collect, index, and visualize audit logs for analysis and investigation.\nAmazon OpenSearch Service Create OpenSearch Domain AWS Console → OpenSearch Service → Create domain.\nSelect Deployment type: Development and testing.\nSet Domain name: audit-trail-demo.\nSet capacity: 1 instance, type c7g.xlarge.search.\nNetwork configuration:\nVPC: AuditDemo-VPC Subnet: AuditDemo-PublicSubnet (Go to Route tables -\u0026gt; Edit subnet associations -\u0026gt; select both Subnets -\u0026gt; click Save) Security Group: AuditDemo-SG Fine-grained access control: Create master user (Create your own username and password) Access policy (allow Lambda and you to access):\n{ \u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;, \u0026ldquo;Statement\u0026rdquo;: [ { \u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;, \u0026ldquo;Principal\u0026rdquo;: { \u0026ldquo;AWS\u0026rdquo;: [ \u0026ldquo;arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:role/AuditLoggerLambdaRole\u0026rdquo;, \u0026ldquo;arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:user/\u0026rdquo; ] }, \u0026ldquo;Action\u0026rdquo;: \u0026ldquo;es:\u0026rdquo;, \u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;arn:aws:es::\u0026lt;ACCOUNT_ID\u0026gt;:domain/audit-trail-demo/\u0026rdquo; } ] }\nClick Create domain and wait for the status to become Active.\nI tried but still don\u0026rsquo;t know how to create the domain correctly.\nOn the Kinesis page, select Create data stream Console → Kinesis Data Firehose → Create Firehose stream. Source: select Amazon Kinesis Data Stream AuditStream (or Direct PUT if not using buffer). Destination: Amazon OpenSearch Service → Domain audit-trail-demo → enter Index name audit-logs. Step 1 must be completed.\nRole: select (or create) IAM Role FirehoseDeliveryRole with es:ESHttpPost and s3:PutObject permissions (if backup is enabled). (Optional) enable S3 backup to save a copy of logs to a bucket like audit-demo-backup. Click Create delivery stream. Connect CloudWatch Logs to Firehose: Console → CloudWatch → Log groups → select /aws/lambda/AuditLoggerDemo → Actions → Stream to AWS service. Select Kinesis Firehose → select stream AuditStream → Save. Create Index Pattern \u0026amp; Visualizations on OpenSearch Dashboards Open OpenSearch Dashboards from the link in the OpenSearch Console.\nGo to Stack Management → Index Patterns → Create index pattern:\nIndex pattern: audit-logs* Select the timestamp field (timestamp or @timestamp) to filter by time. Visualize Library → Create visualization:\nBar chart: count of documents by action field. Date histogram: timeline of events by time frame. Data table: top 10 users with the most events (group by user). Dashboard → Create dashboard Audit Trail Dashboard → Add the visualizations you just created.\nTest \u0026amp; Verify Send some test events via Lambda (step 2) to generate logs.\nWait a few minutes, Firehose will push data to OpenSearch.\nDiscover: on Dashboards, go to the Discover tab to view raw logs.\nDashboard: open Audit Trail Dashboard, check:\nBar chart displays the correct number of events by action. Timeline displays continuously by timestamp. Data table correctly lists the top users. Completing all steps will display the results.\n"
},
{
	"uri": "//localhost:1313/6-cleanup/",
	"title": "Compliance Reporting with Athena",
	"tags": [],
	"description": "",
	"content": "Aggregate and schedule reports from immutable audit logs stored in the S3 bucket audit-demo-logs.\nAmazon Athena Prepare log data on S3 Logs have been written to the audit-demo-logs bucket in the logs/ folder (see step 4). Ensure bucket versioning and Object Lock are still active, so data cannot be deleted/overwritten. Create AWS Glue Crawler AWS Console → Glue → Crawlers → Add crawler. Name: AuditLogsS3Crawler. Data store: select S3, enter path s3://audit-demo-logs/logs/. IAM role: select (or create) GlueCrawlerRole with permissions: JSON:\n{ \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\u0026quot;s3:GetObject\u0026quot;,\u0026quot;s3:ListBucket\u0026quot;],\r\u0026quot;Resource\u0026quot;: [\u0026quot;arn:aws:s3:::audit-demo-logs\u0026quot;,\u0026quot;arn:aws:s3:::audit-demo-logs/logs/*\u0026quot;]\r} Output → Database: audit_reports, Table prefix: audit_logs. Schedule: None (run on demand). After creation, select Run crawler and wait for the status Succeeded. Configure Amazon Athena AWS Console → Athena. Settings (top right) → Query result location: s3://audit-demo-query-results/. In Query Editor, select the database audit_reports. Check the table: SELECT * FROM \u0026ldquo;audit_reports\u0026rdquo;.\u0026ldquo;audit_logsaudit_demo_logs\u0026rdquo; LIMIT 5;\nVerify results Go to S3 audit-demo-query-results/Unsaved -\u0026gt; Check the file "
},
{
	"uri": "//localhost:1313/7-quicksight/",
	"title": "Investigation Tools with Amazon QuickSight",
	"tags": [],
	"description": "",
	"content": "Amazon QuickSight You need to create a QuickSight account to try this demo.\nGrant QuickSight access to data In QuickSight Home, click Manage QuickSight → Security \u0026amp; permissions. In the QuickSight access to AWS services section → click Manage → check Athena → Save. In the QuickSight access to AWS services section → click Select S3 buckets → select audit-demo-logs -\u0026gt; and enable “Write permission for Athena Workgroup”. -\u0026gt; Save. QuickSight now has permission to run queries on Athena and read results in S3. Build Analysis \u0026amp; Visuals From QuickSight Home → click Datasets (left menu) → New dataset. Select Athena → Connect.\nEnter Data source name: AuditLogsAthena → Create data source.\nOn the data selection screen:\nDatabase: audit_reports\nTable: audit_logsaudit_demo_logs\nClick Select → you will see a preview of the table.\nEnable Import to SPICE for quicker analytics → Visualize.\nBuild Analysis \u0026amp; Visuals. Draw Timeline Events: In the Visuals pane, click the + ADD button → select Add visual. Drag the timestamp field from the Data pane to the X axis area in the Field wells pane. The chart will show the number of events over time. Draw Heatmap User Activity: Click + ADD → Add visual. Select the Heat map icon. In Field wells: Drag user to Rows. Drag timestamp to Columns → click the right arrow and change Date granularity to Day. Drag Count to Color intensity. The heatmap will show the density of events per user per day, with color intensity representing the count. Draw Top Users:\nClick + ADD → Add visual. Select the Horizontal bar chart icon.\nIn Field wells:\nDrag user to Category. Drag Count to Value. Add a Filter by action.\nAfter creating all visuals, click Save and name it AuditTrailInvestigation.\nPublish In Analysis, click Publish dashboard.\nDashboard name: Audit Trail Investigation.\n"
},
{
	"uri": "//localhost:1313/8-cloudwatch/",
	"title": "Monitoring &amp; Alerts with Amazon CloudWatch",
	"tags": [],
	"description": "",
	"content": "Amazon CloudWatch When the Lambda audit logs messages containing the keyword “Error”, CloudWatch will automatically count them, and if the threshold is exceeded, it will automatically send alerts via email/Slack.\nCreate Metric Filter Go to CloudWatch Logs\nAWS Console → CloudWatch → on the left select Logs → Log groups. Select the log group for Lambda audit\nClick on /aws/lambda/AuditLoggerDemo. Create metric filter\nOpen the Metric filters tab → click Create metric filter. In the Filter pattern box -\u0026gt; enter ERROR. Set name \u0026amp; configure metric Filter name: AuditErrorFilter Metric namespace: AuditDemo Metric name: ErrorCount Metric value: 1 Click Next → Create filter. Create Alarm Go to CloudWatch Alarms AWS Console → Services → CloudWatch → Alarms → All alarms → Create alarm. Select metric: choose the metric named AuditErrorFilter. Configuration: Statistic: Sum Period: 5 minutes Threshold type: Static Whenever ErrorCount \u0026gt;= 5 For: 1 consecutive period Click Next. Alarm Name: Alarm name: HighErrorAuditAlert Description: \u0026hellip; Click Next. Configure actions: Send notification to: Topic name: AuditAlertsTopic Display name: Audit Alerts Subscription protocol: tranleminhhieu.it@gmail.com (Gmail). Click Next -\u0026gt; Create alarm. Test the Alarm Observe the status: when an error is triggered in Lambda, it will change from OK to In alarm. Check your email to receive the notification. "
},
{
	"uri": "//localhost:1313/9-operationalprocedures/",
	"title": "Operational Procedures",
	"tags": [],
	"description": "",
	"content": "Operational Procedures Ensure your logs are kept for the correct retention period, have backup copies, and have a standard operating procedure (SOP) for incident response.\nConfigure Log Retention Go to CloudWatch Logs\nAWS Console → CloudWatch → on the left select Logs → Log groups. Select the log group for Lambda audit Click on /aws/lambda/AuditLoggerDemo. Click Actions → Edit retention. Choose the retention period (here, select 7 days) → Save. S3 Object Lock\nAWS Console → S3 → click on the audit-demo-logs bucket. Go to the Properties tab → scroll down to Object lock → click Edit Set Retention mode = Compliance (immutable), Retention period = 10 days → Save. Backup \u0026amp; Recovery Create Backup vault\nConsole: AWS Backup → Backup vaults → Create backup vault → Name = AuditBackupVault. Create Backup plan\nConsole → AWS Backup → Backup plans → Create backup plan → Build a new plan. Plan name: AuditBackupPlan Frequency: Daily at 00:00 Retention: 7 days Click Create plan. Assign resource to plan\nConsole: Select AuditBackupPlan → Assign resources → Resource type = S3 → ARN = arn:aws:s3:::audit-demo-logs → Assign. Incident Response SOP Proof of data (S3 Object Lock)\nUse CLI to list versions: aws s3api list-object-versions \u0026ndash;bucket audit-demo-logs \u0026ndash;prefix logs/\nCheck if any version has been deleted or overwritten without authorization. Rebuild QuickSight dashboard\nQuickSight → Datasets → select AuditLogsAthena → click Refresh → wait for SPICE import → Visualize to update the dashboard with new data.\nWith these steps, we have set up retention, backup/recovery, and incident response procedures for the audit system, ensuring it is always safe, recoverable, and operates smoothly.\n"
},
{
	"uri": "//localhost:1313/10-aws-artifact/",
	"title": "Legal Coordination with AWS Artifact",
	"tags": [],
	"description": "",
	"content": "AWS Artifact Collect compliance reports (such as SOC 2, ISO 27001, GDPR) from AWS Artifact to compare and demonstrate that your audit system meets legal standards.\nAccess AWS Artifact AWS Console → Services → Security, Identity, \u0026amp; Compliance → Artifact. On the tab bar, select Reports. Download reports In the Reports tab: Select SOC 2, ISO 27001 then Download (PDF). Save the PDF files to your project\u0026rsquo;s local folder. Compare requirements (Gap Analysis) Open the SOC 2 PDF file Identify requirements for logging, monitoring, retention. Record the mapping between the standards and the implemented features. Gather Audit Trail evidence CloudWatch Logs Export S3 Version List Athena Query Results Snapshot QuickSight Dashboard Compliance PDFs Prepare for submission to regulator Package the files. With this step, we have completed the audit lifecycle from technical (logging, immutability, reporting, dashboard, alerting, SOP) to legal evidence.\n"
},
{
	"uri": "//localhost:1313/11-clean-resource/",
	"title": "Clean Resources",
	"tags": [],
	"description": "",
	"content": "Clean Resources Remove all EC2, VPC, Lambda, S3, OpenSearch, Glue, Athena, Backup, CloudWatch, SNS, EventBridge, etc. used in the demo.\nDelete EC2 \u0026amp; Networking Terminate EC2 Instance\nGo to AWS Console → EC2 → Instances. Select AuditDemo-Server → Actions (at the top) → Instance state → Terminate instance → confirm Terminate. Delete Key Pair\nEC2 → Key Pairs (left menu) → select audit-demo-key → Actions → Delete key pair → Delete. Delete Internet Gateway\nSelect AuditDemo-IGW → Actions → Detach from VPC → confirm → then Actions → Delete internet gateway → Delete. Delete Route Table\nVPC → Route Tables → select AuditDemo-RT → Actions → Delete route table → Delete. Delete Subnets\nVPC → Subnets → select AuditDemo-PublicSubnet and AuditDemo-PublicSubnet2 one by one → for each subnet Actions → Delete subnet → Delete. Delete VPC\nVPC → Your VPCs → select AuditDemo-VPC → Actions → Delete VPC → Delete. Delete S3 Buckets Empty \u0026amp; Delete bucket audit-demo-logs\nAWS Console → S3 → Buckets → select audit-demo-logs. On the Overview tab, click Empty bucket → enter bucket name → Empty. After that, click Delete → enter name → Delete bucket. Empty \u0026amp; Delete bucket audit-demo-query-results\nS3 → select bucket → Empty bucket → confirm → then Delete. Delete Glue, Athena \u0026amp; Backup Delete Glue crawler\nAWS Console → AWS Glue → Crawlers → select AuditLogsS3Crawler → Action → Delete crawler → Delete. Delete Glue database\nGlue → Data Catalog → Databases → select audit_reports → Action → Delete database → Delete. Delete Athena saved queries\nAWS Console → Athena → select Saved queries tab → click saved query name DailyAuditReport → Actions → Delete query → Delete. Delete AWS Backup plan \u0026amp; vault\nAWS Console → AWS Backup → Backup plans → select AuditBackupPlan → Actions → Delete backup plan → Delete. Backup → Backup vaults → select AuditBackupVault → Actions → Delete backup vault → Delete. Delete Lambda \u0026amp; IAM Delete Lambda function\nAWS Console → Lambda → Functions → select AuditLoggerDemo → Actions → Delete function → confirm Delete. Delete IAM Role for Lambda\nConsole → IAM → Roles → find AuditLoggerLambdaRole → click the role → Permissions tab → if there is an inline policy click Delete inline policy to remove → back to role page → Role actions → Delete role → confirm Delete. Delete IAM Role for Glue Crawler\nIAM → Roles → select AWSGlueServiceRole-AuditLogsS3Crawler → Role actions → Delete role → Delete. Delete SNS topic\nAWS Console → SNS → Topics → select AuditAlertsTopic → Actions → Delete topic → Delete. Delete EventBridge rule\nConsole → EventBridge → Rules → select rule DailyAuditReportRule → Actions → Remove targets → tick target → Remove → then Actions → Delete rule → Delete. Delete OpenSearch Domain AWS Console → OpenSearch Service → Domains → select audit-trail-demo → Actions → Delete domain → enter domain name → Delete. Delete CloudWatch Resources Delete Metric filter\nCloudWatch → Logs → Log groups → select /aws/lambda/AuditLoggerDemo → Metric filters tab → select AuditErrorFilter → Actions → Delete filter. Delete Alarm\nCloudWatch → Alarms → All alarms → select HighErrorAuditAlert → Actions → Delete → confirm. Delete Log group\nCloudWatch → Logs → Log groups → select /aws/lambda/AuditLoggerDemo → Actions → Delete log group → Delete. Delete QuickSight Artifacts\nQuickSight → Dashboards → select Audit Trail Investigation → Delete → Delete.\nQuickSight → Analyses → select AuditTrailInvestigation → Delete → Delete.\nQuickSight → Datasets → select AuditLogsAthena → Delete → Delete.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-createiamrole/2.2.2-ec2/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/3.2-private-instance/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Create the source code (audit_logger.py) import json\nimport boto3\nfrom datetime import datetime\n#Initialize QLDB client if needed #qldb_client = boto3.client(\u0026lsquo;qldb\u0026rsquo;)\ndef handler(event, context): payload = { \u0026lsquo;user\u0026rsquo;: event.get(\u0026lsquo;user\u0026rsquo;, \u0026lsquo;anonymous\u0026rsquo;), \u0026lsquo;action\u0026rsquo;: event.get(\u0026lsquo;action\u0026rsquo;, \u0026lsquo;unknown\u0026rsquo;), \u0026rsquo;timestamp\u0026rsquo;: event.get(\u0026rsquo;timestamp\u0026rsquo;, datetime.utcnow().isoformat()) }\n#Write to CloudWatch Logs print(json.dumps(payload)) #Example of writing to QLDB: #qldb_client.send_command(LedgerName=\u0026lsquo;AuditDemoLedger\u0026rsquo;, Statement=\u0026ldquo;INSERT INTO AuditLogs ?\u0026rdquo;, Parameters=[{\u0026lsquo;IonBinary\u0026rsquo;: payload}]) return {\u0026lsquo;status\u0026rsquo;: \u0026lsquo;OK\u0026rsquo;, \u0026lsquo;payload\u0026rsquo;: payload} 2. Packaging zip function.zip audit_logger.py\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]